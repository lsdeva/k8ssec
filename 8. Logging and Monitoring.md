

Set up centralized logging using the EFK stack and implement alerting using Prometheus and Alertmanager.

### Step 1: **Deploy EFK Stack for Centralized Logging**

#### 1. **Deploy Elasticsearch**
Create a file named `elasticsearch-statefulset.yaml`:

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
spec:
  serviceName: elasticsearch
  replicas: 2
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
```

Apply the StatefulSet:

```sh
kubectl apply -f elasticsearch-statefulset.yaml
```

#### 2. **Deploy Fluentd as DaemonSet**
Create a file named `fluentd-daemonset.yaml`:

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
          - name:  FLUENT_ELASTICSEARCH_HOST
            value: "elasticsearch"
          - name:  FLUENT_ELASTICSEARCH_PORT
            value: "9200"
```

Apply the DaemonSet:

```sh
kubectl apply -f fluentd-daemonset.yaml
```

#### 3. **Deploy Kibana**
Create a file named `kibana-deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: kibana:7.10.0
        ports:
        - containerPort: 5601
```

Apply the Deployment:

```sh
kubectl apply -f kibana-deployment.yaml
```

### Step 2: **Deploy Prometheus and Alertmanager for Alerts**

#### 1. **Deploy Prometheus**
You can deploy Prometheus using Helm or manually with YAML files. Here we’ll use Helm.

Add Prometheus Helm repo:

```sh
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
```

Install Prometheus:

```sh
helm install prometheus prometheus-community/prometheus
```

#### 2. **Configure Alertmanager**
Alertmanager configuration can be complex, depending on your needs. You’ll typically create a `ConfigMap` with the Alertmanager configuration and apply it.

Here’s a simplified example:

Create a file named `alertmanager-configmap.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
data:
  config.yml: |
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'email-config'
    receivers:
    - name: 'email-config'
      email_configs:
      - to: '<your-email>'
        send_resolved: true
```

Replace `<your-email>` with your actual email. Apply the ConfigMap:

```sh
kubectl apply -f alertmanager-configmap.yaml
```

### Step 3: **Verification**

#### 1. **Verify EFK Stack**
Check if Elasticsearch, Fluentd, and Kibana pods are running:

```sh
kubectl get pods
```

#### 2. **Verify Prometheus and Alertmanager**
Check if Prometheus and Alertmanager pods are running:

```sh
kubectl get pods
```

### Step 4: **Access Kibana and Prometheus UI**

You may need to expose Kibana and Prometheus UI as services to access them. Create the appropriate service or use port forwarding to access the UI and verify that logs are being collected and alerts are being managed.



